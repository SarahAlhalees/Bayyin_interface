import streamlit as st
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np
import re

# -----------------------------------------
# Streamlit Page Settings
# -----------------------------------------
st.set_page_config(
    page_title="Ø¨ÙÙŠÙÙ‘Ù†Ù’ - Ù…ØµÙ†Ù Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    page_icon="ğŸ“–",
    layout="centered"
)

# -----------------------------------------
# Arabic text normalization
# -----------------------------------------
ARABIC_DIACRITICS = re.compile(r"[\u0617-\u061A\u064B-\u0652]")

def normalize_ar(text):
    text = str(text)
    text = ARABIC_DIACRITICS.sub("", text)
    text = re.sub(r"[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub(r"Ù‰", "ÙŠ", text)
    text = re.sub(r"[Ø¤Ø¦]", "Ø¡", text)
    text = re.sub(r"Ø©", "Ù‡", text)
    text = re.sub(r"[^\w\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# -----------------------------------------
# Load Model
# -----------------------------------------
@st.cache_resource
def load_model():
    repo_id = "SarahAlhalees/Arabertv2_D3Tok"
    subfolder = "Arabertv2_D3Tok"
    tokenizer = AutoTokenizer.from_pretrained(repo_id, subfolder=subfolder)
    model = AutoModelForSequenceClassification.from_pretrained(repo_id, subfolder=subfolder)
    return tokenizer, model

tokenizer, model = load_model()

# -----------------------------------------
# UI Layout
# -----------------------------------------
st.markdown("""
    <h1 style='text-align: center; direction: rtl;'>Ø¨ÙÙŠÙÙ‘Ù†Ù’</h1>
    <h3 style='text-align: center; direction: rtl;'>Ù…ØµÙ†Ù Ù…Ø³ØªÙˆÙ‰ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©</h3>
""", unsafe_allow_html=True)

st.markdown("---")

text = st.text_area(
    label="",
    height=200,
    placeholder="Ø§ÙƒØªØ¨ Ø£Ùˆ Ø§Ù„ØµÙ‚ Ø§Ù„Ù†Øµ Ù‡Ù†Ø§...",
    key="arabic_input"
)

# Add RTL styling for the text area
st.markdown("""
    <style>
    textarea {
        direction: rtl;
        text-align: right;
        font-size: 16px;
    }
    .token-box {
        display: inline-block;
        background-color: #e3f2fd;
        border: 1px solid #90caf9;
        border-radius: 4px;
        padding: 4px 8px;
        margin: 2px;
        font-family: monospace;
        direction: rtl;
    }
    .rtl-text {
        direction: rtl;
        text-align: right;
    }
    </style>
""", unsafe_allow_html=True)

# -----------------------------------------
# Prediction
# -----------------------------------------
if st.button("ğŸ” ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ", use_container_width=True):
    if not text.strip():
        st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ.")
    else:
        cleaned = normalize_ar(text)
        
        inputs = tokenizer(
            cleaned,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        )
        
        with torch.no_grad():
            logits = model(**inputs).logits
        
        probs = torch.softmax(logits, dim=-1).numpy()[0]
        pred_idx = np.argmax(probs)
        level = pred_idx + 1

        # -----------------------------------------
        # Results Section
        # -----------------------------------------
        st.markdown("---")
        st.subheader("ğŸ“Š Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØµÙ†ÙŠÙ")
        
        # Level display with color coding
        level_colors = {1: "ğŸŸ¢", 2: "ğŸŸ¢", 3: "ğŸŸ¡", 4: "ğŸŸ¡", 5: "ğŸ”´", 6: "ğŸ”´"}
        level_names = {
            1: "Ø³Ù‡Ù„ Ø¬Ø¯Ø§Ù‹", 2: "Ø³Ù‡Ù„", 3: "Ù…ØªÙˆØ³Ø·", 
            4: "ØµØ¹Ø¨ Ù‚Ù„ÙŠÙ„Ø§Ù‹", 5: "ØµØ¹Ø¨", 6: "ØµØ¹Ø¨ Ø¬Ø¯Ø§Ù‹"
        }
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric(label="Ø§Ù„Ù…Ø³ØªÙˆÙ‰", value=f"{level_colors.get(level, 'âšª')} {level}")
        with col2:
            st.metric(label="Ø§Ù„ÙˆØµÙ", value=level_names.get(level, "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"))
        
        st.progress(int(probs[pred_idx] * 100))
        st.write(f"**Ù†Ø³Ø¨Ø© Ø§Ù„Ø«Ù‚Ø©:** {probs[pred_idx]:.2%}")

        # -----------------------------------------
        # Tokenization Section
        # -----------------------------------------
        st.markdown("---")
        st.subheader("ğŸ”¤ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª (Tokenization)")
        
        # Get tokens
        token_ids = inputs["input_ids"][0].tolist()
        tokens = tokenizer.convert_ids_to_tokens(token_ids)
        
        # Filter out special tokens for display
        special_tokens = [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]
        display_tokens = [(tok, tid) for tok, tid in zip(tokens, token_ids) if tok not in special_tokens]
        
        # Token statistics
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª", len(display_tokens))
        with col2:
            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª (Ù…Ø¹ Ø§Ù„Ø®Ø§ØµØ©)", len(tokens))
        with col3:
            word_count = len(cleaned.split())
            st.metric("Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª", word_count)
        
        # Display tokens visually
        st.write("**Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª:**")
        token_html = '<div class="rtl-text" style="line-height: 2.5;">'
        for tok, tid in display_tokens:
            # Clean token display (remove ## prefix for subwords)
            display_tok = tok.replace("##", "")
            token_html += f'<span class="token-box" title="ID: {tid}">{display_tok}</span>'
        token_html += '</div>'
        st.markdown(token_html, unsafe_allow_html=True)
        
        # Expandable section for detailed token info
        with st.expander("ğŸ“‹ Ø¹Ø±Ø¶ ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª"):
            import pandas as pd
            token_data = {
                "Ø§Ù„ØªÙˆÙƒÙ†": [tok for tok, _ in display_tokens],
                "Token ID": [tid for _, tid in display_tokens],
                "Ù†ÙˆØ¹": ["Ø¬Ø²Ø¡ Ù…Ù† ÙƒÙ„Ù…Ø©" if tok.startswith("##") else "ÙƒÙ„Ù…Ø©/Ø¨Ø¯Ø§ÙŠØ©" for tok, _ in display_tokens]
            }
            df = pd.DataFrame(token_data)
            st.dataframe(df, use_container_width=True, hide_index=True)
        
        # -----------------------------------------
        # Processed Text Section
        # -----------------------------------------
        st.markdown("---")
        st.subheader("ğŸ”§ Ø§Ù„Ù†Øµ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©")
        st.markdown(f'<div class="rtl-text" style="background-color: #f5f5f5; padding: 15px; border-radius: 8px;">{cleaned}</div>', unsafe_allow_html=True)

# Footer
st.markdown("---")
st.caption("Â© 2025 â€” Ù…Ø´Ø±ÙˆØ¹ Ø¨ÙÙŠÙÙ‘Ù†Ù’")
